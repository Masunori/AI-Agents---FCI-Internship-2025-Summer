**Bản tin công nghệ FPT Smart Cloud – Tháng 9/2025**  
*“Tiên phong giảm chi phí và mở rộng khả năng thực thi của AI trên đám mây”*  

---

### Mở đầu  
Trong tuần qua cộng đồng nghiên cứu đã công bố một loạt bài báo quan trọng liên quan đến **trí tuệ nhân tạo (Artificial Intelligence – AI)** và **điện toán đám mây (Cloud Computing)**. Các kết quả này hứa hẹn giảm đáng kể nhu cầu tài nguyên khi triển khai mô hình ngôn ngữ lớn (Large Language Models – LLMs), nâng cao khả năng thực hiện các nhiệm vụ dài hạn và cải thiện hiệu quả học tăng cường (Reinforcement Learning – RL). Dưới đây là những điểm nổi bật được sắp xếp theo mức độ liên quan tới chiến lược phát triển của FPT Smart Cloud.

---

## 1. ButterflyQuant: Siêu‑low‑bit Quantization cho LLM bằng Transform Butterfly có thể học  

- **Nguồn & thời gian:** arXiv, 11 / 09 / 2025  
- **Tóm tắt ngắn gọn:**  
  Các mô hình ngôn ngữ lớn thường cần hàng chục gigabyte bộ nhớ, khiến việc chạy chúng trên máy tính cá nhân hoặc thiết bị biên khó khăn. *Quantization* (giảm độ chính xác số) giúp tiết kiệm bộ nhớ nhưng khi chỉ dùng 2‑bit sẽ gây “mất mát thảm họa” vì các giá trị ngoại lệ (outliers). Các phương pháp trước như QuIP hay QuaRot dùng phép quay cố định (Hadamard matrix) để loại bỏ ngoại lệ trước khi lượng tử hoá, nhưng không thể thích nghi với từng lớp của mô hình.  

  **ButterflyQuant** thay thế phép quay cố định bằng *Butterfly Transform* có thể học được thông qua các góc quay Givens liên tục. Nhờ vậy:
  - Phép biến đổi vẫn luôn trực giao (orthogonal), bảo đảm không làm mất thông tin quan trọng.  
  - Chỉ cần **O(n log n)** tính toán và khoảng **n log n / 2** tham số học được – rất nhẹ.  
  - Với chỉ 128 mẫu hiệu chuẩn và vài phút đào tạo trên một GPU, mô hình LLaMA‑2‑7B được lượng tử hoá 2‑bit đạt perplexity **15.4**, tốt hơn hẳn QuaRot (**22.1**).  

- **Ý nghĩa thực tiễn:**  
  - Giảm tới **90 %** dung lượng lưu trữ so với phiên bản đầy đủ, cho phép triển khai LLM trực tiếp trên máy khách hoặc môi trường edge mà không cần server mạnh.  
  - Thời gian khởi động nhanh hơn → giảm chi phí vận hành cloud cho các dịch vụ chatbot hay trợ lý ảo nội bộ của doanh nghiệp.  

---

## 2. The Illusion of Diminishing Returns: Đánh giá khả năng thực thi dài hạn của LLM  

- **Nguồn & thời gian:** arXiv, 11 / 09 / 2025  
- **Tóm tắt ngắn gọn:**  
  Nhiều người cho rằng việc mở rộng quy mô LLM sẽ gặp “điểm bão hòa”. Bài báo chỉ ra rằng dù độ chính xác từng bước chỉ tăng nhẹ, nhưng khi cộng dồn qua nhiều bước thì khả năng hoàn thành một nhiệm vụ dài lại tăng **bậc hàm mũ**. Nguyên nhân không phải do thiếu khả năng suy luận mà là lỗi trong *execution* (thực thi) – mỗi lần mô hình sai sẽ “làm ô nhiễm” ngữ cảnh cho bước kế tiếp (hiệu ứng *self‑conditioning*). Hiện tượng này không giảm ngay cả khi tăng kích thước mô hình; tuy nhiên các mô hình “thinking” mới không tự‑condition và có thể hoàn thành tác vụ dài trong một lượt duy nhất.

- **Ý nghĩa thực tiễn:**  
  - Khi xây dựng dịch vụ AI hỗ trợ quy trình kinh doanh kéo dài nhiều vòng tương tác (ví dụ: hỗ trợ khách hàng đa bước), cần chú trọng vào cơ chế kiểm tra và sửa lỗi giữa các vòng thay vì chỉ tối ưu độ chính xác từng câu trả lời riêng lẻ.  
  - Đưa ra hướng phát triển: kết hợp *self‑correction* hoặc *chain‑of‑thought prompting* để giảm ảnh hưởng của lỗi tích luỹ – điều này có thể được triển khai như một micro‑service trên nền tảng cloud của FCI.

---

## 3. CDE – Curiosity‑Driven Exploration cho Reinforcement Learning trong LLM  

- **Nguồn & thời gian:** arXiv, 11 / 09 / 2025  
- **Tóm tắt ngắn gọn:**  
  Học tăng cường với phần thưởng kiểm chứng (*Reinforcement Learning with Verifiable Rewards – RLVR*) giúp LLM cải thiện khả năng lập luận nhưng thường “bị kẹt” do khám phá kém. *CDE* đề xuất sử dụng sự tò mò nội tại của mô hình làm phần thưởng phụ:
  - Đối với *actor*, đo độ bất định bằng **perplexity** của phản hồi sinh ra.  
  - Đối với *critic*, đo biến thiên giá trị dự đoán qua kiến trúc đa đầu (*multi‑head*).  

  Hai tín hiệu này tạo ra “bonus khám phá”, khuyến khích mô hình đưa ra câu trả lời đa dạng và tránh quá tự tin sai lầm. Trên bộ benchmark AIME, CDE cải thiện khoảng **+3 điểm** so với RLVR tiêu chuẩn.

- **Ý nghĩa thực tiễn:**  
  - Áp dụng vào việc fine‑tune các chatbot doanh nghiệp để chúng không chỉ trả lời đúng mà còn đưa ra nhiều cách giải quyết khác nhau – tăng trải nghiệm người dùng và giảm rủi ro trả lời sai lặp lại.  
  - Kiến trúc bonus tò mò có thể được đóng gói thành một plugin trên nền tảng AI-as-a-Service của FCI, giúp khách hàng nhanh chóng nâng cấp mô hình mà không cần thu thập thêm dữ liệu lớn.

---

## 4. SimpleVLA‑RL: Tăng tốc huấn luyện Vision‑Language‑Action bằng Reinforcement Learning  

- **Nguồn & thời gian:** arXiv, 11 / 09 / 2025  
- **Tóm tắt ngắn gọn:**  
  Các mô hình *Vision‑Language‑Action* (VLA) dùng để điều khiển robot thường gặp hai rào cản: thiếu dữ liệu thao tác con người quy mô lớn và khó tổng quát khi môi trường thay đổi. *SimpleVLA‑RL* giới thiệu một khuôn khổ RL tối ưu cho VLA:
  - Sử dụng kỹ thuật lấy mẫu đường đi đặc thù VLA và render đa môi trường song song.
  - Tối ưu hoá tính toán loss để giảm chi phí GPU.
   
   Khi áp dụng lên OpenVLA-OFT, phương pháp đạt thành tích tốt nhất trên tập LIBERO và vượt cả \(\pi_0\) trên RoboTwin 1.0 & 2.0 — đồng thời giảm đáng kể nhu cầu dữ liệu huấn luyện truyền thống.

- **Ý nghĩa thực tiễn:**  
  - Giúp các nhà phát triển giải pháp robot tự động hoá trong nhà máy hoặc kho hàng triển khai nhanh hơn mà không phải thu thập hàng triệu giờ video thao tác thực tế.
  - Công nghệ này có thể được cung cấp dưới dạng dịch vụ *Robotics Cloud* của FCI, nơi khách hàng gửi dữ liệu môi trường lên đám mây để nhận lại policy robot đã được tối ưu sẵn.

---

## 5. FG‑FARL: Học tăng cường offline công bằng cho quản lý chăm sóc y tế Medicaid  

- **Nguồn & thời gian:** arXiv, 11 / 09 / 2025  
- **Tóm tắt ngắn gọn:**  
  Trong bối cảnh y tế công cộng, việc đưa ra quyết định hỗ trợ bệnh nhân phải cân bằng giữa hiệu quả và công bằng xã hội. *Feasibility‑Guided Fair Adaptive Offline Reinforcement Learning (FG‑FARL)* đề xuất:
   - Xác định mức an toàn riêng cho từng nhóm dân cư nhằm giảm thiểu tổn thương.
   - Đảm bảo đồng đều mục tiêu công bằng (coverage hoặc harm) giữa các nhóm bảo vệ.
   
   Thử nghiệm trên dữ liệu Medicaid cho thấy FG‑FARL đạt giá trị chính sách tương đương baseline nhưng cải thiện đáng kể các chỉ số công bằng.

- **Ý nghĩa thực tiễn:**  
   - Mô hình có thể được tích hợp vào hệ thống quyết định hỗ trợ y tế dựa trên cloud để cung cấp khuyến nghị điều trị công bằng hơn mà không cần thu thập thêm dữ liệu nhạy cảm.
   - Đối với FCI, đây là ví dụ tiêu biểu về cách áp dụng AI an toàn (*secure AI*) trong lĩnh vực y tế – mở ra cơ hội hợp tác với các đối tác chăm sóc sức khỏe quốc gia.

---

### Kết luận  
Các nghiên cứu vừa nêu đều hướng tới ba mục tiêu then chốt mà FPT Smart Cloud đang theo đuổi:

1️⃣ **Giảm chi phí tài nguyên** – qua kỹ thuật quantization siêu thấp bit giúp LLM chạy trên thiết bị biên hoặc môi trường cloud tiết kiệm RAM/GPU.<br>
2️⃣ **Nâng cao khả năng thực thi dài hạn** – hiểu rõ nguồn gốc lỗi chuỗi bước giúp thiết kế hệ thống kiểm soát lỗi tự động.<br>
3️⃣ **Cải thiện chất lượng học tăng cường** – từ khám phá tò mò đến đào tạo robot VLA hiệu quả hơn và đảm bảo công bằng trong quyết định y tế.

Những tiến bộ này sẽ là nền tảng vững chắc để chúng ta tiếp tục cung cấp dịch vụ AI mạnh mẽ, linh hoạt và an toàn trên nền tảng đám mây hiện đại.

---  

#### Tham khảo

| Bài báo | Link |
|---|---|
| ButterflyQuant | <http://arxiv.org/pdf/2509.09679v1> |
| The Illusion of Diminishing Returns | <http://arxiv.org/pdf/2509.09677v1> |
| CDE – Curiosity-Driven Exploration | <http://arxiv.org/pdf/2509.09675v1> |
| SimpleVLA-RL | <http://arxiv.org/pdf/2509.09674v1> |
| FG-FARL | <http://arxiv.org/pdf/2509.09655v1> |

---  