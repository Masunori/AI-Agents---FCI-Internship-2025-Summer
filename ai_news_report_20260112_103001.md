# Bản tin công nghệ - 12/01/2026

## Mở đầu

Tuần này FPT Smart Cloud được chiếu sáng bởi một loạt đột phá công nghệ từ NVIDIA và các nhà nghiên cứu AI tiên phong. Từ lớp bộ nhớ "G3.5" – Inference Context Memory Storage (ICMS) dựa trên BlueField‑4 mang lại KV cache tốc terabit cho các mô hình LLM siêu dài – tới Spectrum‑X Ethernet Photonics giảm tiêu thụ năng lượng năm lần cho mỗi cổng 1.6 Tb/s; từ kiến trúc Blackwell tăng gấp 2.8× thông lượng token trên MoE siêu lớn – đến TensorRT Edge‑LLM mở cửa suy luận nhanh trên thiết bị nhúng ô tô và robot. Song song đó, Netomi giới thiệu agent AI doanh nghiệp dựa trên GPT‑4.1/5.2 với khả năng concurrency và multi‑step reasoning; các nghiên cứu MICL hứa hẹn mở rộng ASR đa ngôn ngữ tài nguyên thấp; LoopBench phát hiện vòng lặp "Circular Reasoning"; AGDC tái định nghĩa sinh chuỗi đa dạng cho thiết kế bán dẫn; Stephanie2 tinh chỉnh nhịp điệu hội thoại; và HAG xây dựng cây nhân khẩu học để mô phỏng người dùng chính xác hơn. Tất cả đều vẽ nên bức tranh công nghệ đa chiều mà FPT Smart Cloud có thể khai thác để nâng cao hiệu suất, giảm chi phí và mở rộng quy mô dịch vụ AI.

## Điểm nhấn: Introducing NVIDIA BlueField-4-Powered Inference Context Memory Storage Platform for the Next Frontier of AI

NVIDIA BlueField‑4‑powered Inference Context Memory Storage (ICMS) tạo ra lớp “G3.5” mới – một tầng bộ nhớ flash gắn Ethernet được tối ưu cho KV cache – giúp lấp đầy khoảng trống giữa GPU HBM tốc nan giây và lưu trữ G4 có độ trễ mili giây. Nhờ DPU BlueField‑4 và Spectrum‑X RDMA, ICMS cung cấp băng thông hàng terabit cùng khả năng chia sẻ petabyte KV cache giữa các nút trong pod, đạt tới 5 × tốc độ token‑per‑second và 5 × hiệu suất năng lượng so với lưu trữ truyền thống. Đối với FPT Smart Cloud, công nghệ này cho phép triển khai các mô hình LLM siêu dài (hàng triệu token) mà không làm nghẽn GPU, giảm đáng kể chi phí điện năng và chi phí mỗi token. Khi KV cache được lưu trữ ở tầng G3.5 gần GPU, các dịch vụ AI agentic của FPT có thể tái sử dụng ngữ cảnh lâu dài, tăng khả năng đồng thời chạy nhiều tác vụ và rút ngắn độ trễ tail. Nhờ đó FPT có thể nâng cao TCO của trung tâm dữ liệu AI, mở rộng quy mô dịch vụ đám mây AI một cách bền vững và nhanh chóng đáp ứng nhu cầu khách hàng doanh nghiệp trong kỷ nguyên AI đa tác vụ.

**Ngày xuất bản:** 06 Jan, 2026
**URL:** https://developer.nvidia.com/blog/introducing-nvidia-bluefield-4-powered-inference-context-memory-storage-platform-for-the-next-frontier-of-ai/

## Mục 1: Netomi’s lessons for scaling agentic systems into the enterprise

Trong bản cập nhật này, Netomi đã triển khai các agent AI doanh nghiệp dựa trên GPT‑4.1 và GPT‑5.2, tích hợp khả năng xử lý đồng thời (concurrency), khung quản trị (governance) chặt chẽ và quy trình suy luận đa bước (multi‑step reasoning). Nhờ việc kết hợp ba yếu tố này, hệ thống có thể duy trì luồng công việc sản xuất ổn định, giảm thiểu lỗi khi thực thi các tác vụ phức tạp và đáp ứng yêu cầu thời gian thực của môi trường doanh nghiệp quy mô lớn.

Đối với FPT Smart Cloud, kiến trúc này mở ra cơ hội nâng cấp nền tảng AI nội bộ, cho phép chúng ta triển khai các dịch vụ chatbot và trợ lý ảo có khả năng mở rộng mạnh mẽ mà không làm mất kiểm soát an ninh hay chất lượng quyết định. Việc áp dụng mô hình quản trị và suy luận đa bước sẽ giúp chúng ta cung cấp giải pháp AI đáng tin cậy cho khách hàng doanh nghiệp, tăng tính cạnh tranh và giảm chi phí vận hành hệ thống AI quy mô toàn cầu.

**Ngày xuất bản:** 08 Jan, 2026
**URL:** https://openai.com/index/netomi

## Mục 2: Accelerating LLM and VLM Inference for Automotive and Robotics with NVIDIA TensorRT Edge-LLM

TensorRT Edge‑LLM là khung C++ mã nguồn mở mới cho phép suy luận nhanh chóng các mô hình LLM và VLM trên nền tảng nhúng ô tô và robot (NVIDIA DRIVE AGX Thor, Jetson Thor). Khác với các giải pháp tập trung vào trung tâm dữ liệu, nó cung cấp các tính năng tiên tiến như giải mã dự đoán EAGLE‑3, lượng tử hoá NVFP4 và tiền tải dạng khối (chunked prefill), đồng thời giảm thiểu phụ thuộc và dung lượng tài nguyên nhờ thiết kế nhẹ nhàng và quy trình từ Hugging Face → ONNX → TensorRT engine.

Đối với FPT Smart Cloud, việc tích hợp TensorRT Edge‑LLM mở ra khả năng triển khai AI trên thiết bị với độ trễ dự đoán thấp và hoạt động offline – yếu tố then chốt cho các dự án xe tự lái, robot công nghiệp và hệ thống cabin thông minh. Nhờ vậy chúng ta có thể cung cấp dịch vụ đám mây biên (edge cloud) cho khách hàng trong ngành ô tô và robotics, tăng tính cạnh tranh và tạo tiền đề cho hợp tác chiến lược với các nhà cung cấp phần cứng như Bosch hay MediaTek.

**Ngày xuất bản:** 08 Jan, 2026
**URL:** https://developer.nvidia.com/blog/accelerating-llm-and-vlm-inference-for-automotive-and-robotics-with-nvidia-tensorrt-edge-llm/

## Mục 3: Scaling Power-Efficient AI Factories with NVIDIA Spectrum-X Ethernet Photonics

NVIDIA giới thiệu Spectrum‑X Ethernet Photonics – switch co‑packaged optics đầu tiên thế giới, tích hợp 512 lane 200 G, giảm tiêu thụ năng lượng tới 5× cho mỗi cổng 1.6 Tb/s và kéo dài thời gian hoạt động không lỗi gấp 5× so với Ethernet truyền thống. Thiết kế mới gồm kết nối sợi quang tháo rời, cơ chế shuffle nội bộ và động cơ quang học tương thích hàn lại, cho phép lắp ráp tự động, kiểm tra trước khi gắn và đạt tỷ lệ thành công 100 %. Đối với FPT Smart Cloud, công nghệ này hứa hẹn giảm chi phí điện năng của các AI factory quy mô giga‑scale, tăng độ tin cậy cho các dịch vụ AI đa người dùng và rút ngắn thời gian triển khai hạ tầng mạng siêu tốc, từ đó nâng cao khả năng cạnh tranh trong thị trường đám mây AI nội địa và quốc tế.

**Ngày xuất bản:** 06 Jan, 2026
**URL:** https://developer.nvidia.com/blog/scaling-power-efficient-ai-factories-with-nvidia-spectrum-x-ethernet-photonics/

## Mục 4: Delivering Massive Performance Leaps for Mixture of Experts Inference on NVIDIA Blackwell

Các bản cập nhật mới nhất của phần mềm suy luận NVIDIA và kiến trúc Blackwell đã giới thiệu một loạt cải tiến quan trọng: dự đoán đa‑token (MTP), định dạng số thực bốn bit NVFP4, kiến trúc phục vụ tách rời (disaggregated serving) và các primitive giao tiếp all‑to‑all được tối ưu hoá. Kết hợp với việc mở rộng Programmatic Dependent Launch và tối ưu kernel sâu hơn, mỗi GPU Blackwell hiện đạt tăng tốc tới 2.8× về thông lượng token trên các mô hình MoE siêu lớn như DeepSeek‑R1. Đối với FPT, những cải tiến này giảm đáng kể chi phí mỗi triệu token và nâng cao năng suất watt‑per‑token, cho phép hạ tầng GPU hiện có kéo dài tuổi thọ và đáp ứng nhanh hơn các dịch vụ AI tương tác cao – từ nền tảng đám mây nội bộ tới các giải pháp doanh nghiệp yêu cầu xử lý mô hình ngôn ngữ quy mô siêu lớn.

**Ngày xuất bản:** 08 Jan, 2026
**URL:** https://developer.nvidia.com/blog/delivering-massive-performance-leaps-for-mixture-of-experts-inference-on-nvidia-blackwell/

## Mục 5: Redefining Secure AI Infrastructure with NVIDIA BlueField Astra for NVIDIA Vera Rubin NVL72

NVIDIA BlueField Astra chạy trên BlueField‑4 mang lại kiến trúc kiểm soát thống nhất cho cả mạng North‑South và East‑West, cho phép DPU lập trình trực tiếp các SuperNIC ConnectX‑9 qua cổng out‑of‑band. Nhờ đó mọi chính sách mạng, phân vùng tài nguyên và bảo mật được thực thi hoàn toàn trên DPU, tách biệt hoàn toàn khỏi môi trường tenant bare‑metal; các microservice DOCA (HBN, OVS, Argus, SNAP, DMS) cũng được mở rộng vào fabric tính toán AI. Điều này tạo ra khả năng cung cấp GPU bare‑metal đa thuê với cách ly nghiêm ngặt, giảm độ trễ cấu hình và cung cấp dấu vết audit rõ ràng—yếu tố then chốt cho các dịch vụ AI quy mô lớn và yêu cầu tuân thủ quy định. Đối với FPT Smart Cloud, công nghệ này giúp chúng ta nhanh chóng triển khai hạ tầng AI an toàn, tối ưu chi phí vận hành và nâng cao niềm tin khách hàng trong môi trường đa‑tenant.

**Ngày xuất bản:** 07 Jan, 2026
**URL:** https://developer.nvidia.com/blog/redefining-secure-ai-infrastructure-with-nvidia-bluefield-astra-for-nvidia-vera-rubin-nvl72/

## Mục 6: Introducing NVIDIA BlueField-4-Powered Inference Context Memory Storage Platform for the Next Frontier of AI

NVIDIA giới thiệu nền tảng Inference Context Memory Storage (ICMS) dựa trên bộ xử lý BlueField‑4, tạo lớp “G3.5” flash‑đính Ethernet trong kiến trúc pod Rubin. Điểm mới là việc đưa KV cache – bộ nhớ ngữ cảnh quan trọng cho các mô hình LLM siêu dài – ra khỏi HBM và SSD truyền thống, chuyển sang tầng lưu trữ tốc độ cao, tiêu thụ ít năng lượng và hỗ trợ RDMA qua Spectrum‑X. Điều này giúp giảm tới 5 × thời gian truy cập và tiêu thụ năng lượng so với lưu trữ chung, đồng thời tăng 5 × token‑per‑second cho các workload agentic.

Với FPT Smart Cloud, ICMS mở ra khả năng mở rộng AI inference mà không cần gia tăng GPU hay nâng cấp hạ tầng lưu trữ truyền thống; chúng ta có thể phục vụ các ứng dụng chatbot đa vòng, công cụ AI tự động hoá và phân tích dữ liệu siêu lớn với chi phí điện năng thấp hơn và hiệu suất cao hơn, đồng thời tối ưu TCO cho các trung tâm dữ liệu nội bộ.

**Ngày xuất bản:** 06 Jan, 2026
**URL:** https://developer.nvidia.com/blog/introducing-nvidia-bluefield-4-powered-inference-context-memory-storage-platform-for-the-next-frontier-of-ai/

## Mục 7: Redefining Secure AI Infrastructure with NVIDIA BlueField Astra for NVIDIA Vera Rubin NVL72

BlueField Astra giới thiệu kiến trúc Trusted Resource mới chạy trên DPU BlueField‑4, cho phép DPU kiểm soát trực tiếp cả hai miền mạng North‑South và East‑West thông qua kết nối riêng với SuperNIC ConnectX‑9. Nhờ đường truyền out‑of‑band và việc di chuyển toàn bộ stack DOCA sang DPU, các chính sách cách ly và bảo mật được áp dụng ở mức phần cứng cho các nút GPU bare‑metal mà không thể bị thay đổi bởi tenant.

Đối với FPT Smart Cloud, mô hình này cung cấp nền tảng an toàn để triển khai dịch vụ AI đa khách hàng trên hạ tầng đám mây riêng mà vẫn duy trì hiệu năng GPU tối đa. Việc đồng nhất công cụ quản lý N‑S và E‑W giảm độ phức tạp vận hành, tăng khả năng audit và đáp ứng yêu cầu tuân thủ của các ngành tài chính hay y tế – những thị trường chiến lược của chúng ta.

**Ngày xuất bản:** 07 Jan, 2026
**URL:** https://developer.nvidia.com/blog/redefining-secure-ai-infrastructure-with-nvidia-bluefield-astra-for-nvidia-vera-rubin-nvl72/

## Mục 8: Introducing NVIDIA BlueField-4-Powered Inference Context Memory Storage Platform for the Next Frontier of AI

NVIDIA giới thiệu nền tảng Inference Context Memory Storage (ICMS) dựa trên bộ xử lý BlueField‑4, tạo lớp “G3.5” – một tầng bộ nhớ flash gắn Ethernet chuyên phục vụ KV cache trong các mô hình LLM siêu dài (hàng triệu token). ICMS nối liền GPU HBM và lưu trữ chia sẻ, cung cấp băng thông cao, độ trễ thấp và hiệu suất năng lượng gấp 5 lần so với lưu trữ truyền thống, đồng thời mở rộng khả năng lưu trữ KV lên quy mô petabyte cho mỗi pod. Đối với FPT Smart Cloud, công nghệ này cho phép mở rộng các dịch vụ AI agentic mà không phải tăng đáng kể công suất GPU hay chi phí điện năng; chúng ta có thể tái sử dụng ngữ cảnh lâu dài, giảm thời gian dừng decode và nâng cao TPS, từ đó cải thiện TCO và đáp ứng nhu cầu khách hàng doanh nghiệp muốn triển khai mô hình LLM siêu lớn trên hạ tầng đám mây nội bộ.

**Ngày xuất bản:** 06 Jan, 2026
**URL:** https://developer.nvidia.com/blog/introducing-nvidia-bluefield-4-powered-inference-context-memory-storage-platform-for-the-next-frontier-of-ai/

## Mục 9: Scaling Power-Efficient AI Factories with NVIDIA Spectrum-X Ethernet Photonics

Sự ra mắt NVIDIA Spectrum‑X Ethernet Photonics mang lại ba đổi mới cốt lõi: (1) công nghệ co‑packaged silicon photonics giảm 5 lần tiêu thụ điện năng cho mỗi cổng 1.6 Tb/s và kéo dài thời gian hoạt động không gián đoạn gấp 5; (2) kiến trúc switch tích hợp 512 lane 200 G với cơ chế “fiber shuffle” cho băng thông tổng 409.6 Tb/s và khả năng mở rộng phẳng cho cụm GPU; (3) đầu nối quang học tháo rời và động cơ quang tương thích hàn‑reflow cho quy trình lắp ráp tự động đạt 100 % yield. Đối với FPT Smart Cloud, những cải tiến này giảm đáng kể OPEX nhờ năng lượng thấp hơn, nâng cao độ tin cậy của hạ tầng AI quan trọng cho các dịch vụ đám mây và AI nội bộ, đồng thời tạo nền tảng mạng có khả năng mở rộng nhanh chóng đáp ứng nhu cầu tăng trưởng dữ liệu và mô hình siêu lớn trong tương lai.

**Ngày xuất bản:** 06 Jan, 2026
**URL:** https://developer.nvidia.com/blog/scaling-power-efficient-ai-factories-with-nvidia-spectrum-x-ethernet-photonics/

## Mục 10: Multimodal In-context Learning for ASR of Low-resource Languages

Bản nghiên cứu giới thiệu **Multimodal In‑context Learning (MICL) cho nhận dạng giọng nói** ở các ngôn ngữ tài nguyên thấp – một phương pháp chưa từng được áp dụng trong các mô hình LLM âm thanh hiện nay. Thay vì chỉ dùng văn bản làm ví dụ, MICL kết hợp cả âm thanh và bản ghi chú trong prompt, cho phép mô hình học ngôn ngữ chưa từng gặp chỉ với vài mẫu đa phương tiện và cải thiện đáng kể độ chính xác khi tái‑xếp hạng các giả thuyết của mô hình acoustic truyền thống. Đối với FPT Smart Cloud, khả năng nhanh chóng mở rộng dịch vụ ASR sang hàng nghìn ngôn ngữ địa phương mà không cần thu thập dữ liệu nhãn quy mô lớn sẽ giảm chi phí triển khai, tăng tính cạnh tranh trên thị trường Đông Nam Á và hỗ trợ chiến lược “đi tới mọi tiếng nói” của công ty.

**Ngày xuất bản:** 09 Jan, 2026
**URL:** https://arxiv.org/pdf/2601.05707v1

## Mục 11: Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models

Bài nghiên cứu vừa công bố xác định một dạng lỗi mới trong các Large Reasoning Models (LRM) – “Circular Reasoning”. Đây là hiện tượng vòng lặp tự‑tăng cường, khi nội dung đã sinh ra trở thành tiền đề logic cho chính nó, dẫn tới việc mô hình lặp lại câu/số liên tục và không thể thoát ra. Nghiên cứu cung cấp LoopBench, bộ dữ liệu chuẩn đoán hai kiểu vòng lặp (numeric và statement), đồng thời mô tả cơ chế “V‑shaped” attention và dấu hiệu sớm bằng thuật toán CUSUM. Đối với FPT Smart Cloud, việc nhận diện và dự báo sớm các vòng lặp này giúp giảm đáng kể chi phí tính toán, ngăn ngừa treo hệ thống trong các dịch vụ AI đám mây và nâng cao độ tin cậy của các giải pháp suy luận phức tạp mà chúng ta triển khai cho khách hàng.

**Ngày xuất bản:** 09 Jan, 2026
**URL:** https://arxiv.org/pdf/2601.05693v1

## Mục 12: AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces

AGDC mang lại khả năng sinh chuỗi độ dài biến đổi đồng thời xử lý giá trị rời rạc và liên tục trong một mô hình tự hồi quy duy nhất. Thay vì chuyển đổi các giá trị liên tục thành token rời rạc – gây mất độ chính xác và bùng nổ từ vựng – AGDC kết hợp dự đoán phân loại cho phần rời rạc và mô hình khuếch tán cho phần liên tục, đồng thời bổ sung cơ chế điều chỉnh logit EOS dựa trên MLP và hàm phạt chuẩn hoá độ dài. Điều này cho phép tạo ra các bản thiết kế bán dẫn, layout đồ họa và SVG với độ chính xác vô hạn, tránh lỗi vị trí nanomet‑level có thể làm hỏng mạch điện tử. Đối với FPT Smart Cloud, công nghệ này mở đường cho các dịch vụ thiết kế vi mạch tự động, tối ưu hoá tài nguyên đám mây và cung cấp giải pháp AI cao‑độ chính xác cho khách hàng trong lĩnh vực bán dẫn và thiết kế UI/UX, nâng cao năng lực cạnh tranh trên thị trường công nghệ tiên tiến.

**Ngày xuất bản:** 09 Jan, 2026
**URL:** https://arxiv.org/pdf/2601.05680v1

## Mục 13: Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat

Stephanie2 mang lại hai cải tiến cốt lõi so với phiên bản trước: **chế độ chờ chủ động** cho phép hệ thống quyết định “gửi” hay “đợi” dựa trên ngữ cảnh, và **cơ chế điều chỉnh tốc độ tin nhắn** tính cả thời gian suy nghĩ và thời gian gõ. Nhờ đó mô hình mô phỏng nhịp điệu trò chuyện thực tế hơn, giảm hiện tượng ngắt lời khi người dùng đang diễn đạt cảm xúc và tạo ra khoảng trễ hợp lý cho các phản hồi ngắn hoặc dài. Đối với FPT Smart Cloud, những tiến bộ này giúp nâng cao trải nghiệm AI đồng hành và hỗ trợ cảm xúc trong các sản phẩm chatbot doanh nghiệp, tăng mức độ tự nhiên và gắn kết người dùng – yếu tố then chốt để duy trì lợi thế cạnh tranh trên thị trường dịch vụ đám mây và AI tại Việt Nam.

**Ngày xuất bản:** 09 Jan, 2026
**URL:** https://arxiv.org/pdf/2601.05657v1

## Mục 14: HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation

HAG giới thiệu một khung tạo đại diện người dùng dựa trên **cây phân cấp nhân khẩu học** (Hierarchical Demographic Tree) cho phép mô phỏng đa dạng theo chủ đề (topic‑adaptive). Khác với các phương pháp truy xuất dữ liệu tĩnh hoặc sinh nhân vật bằng LLM riêng lẻ, HAG kết hợp hai giai đoạn: (1) xây dựng cây phân phối điều kiện dựa trên mô hình Kiến thức Thế giới (World Knowledge Model) để nắm bắt các phụ thuộc đa chiều giữa các thuộc tính; (2) khớp cây với dữ liệu thực tế và bổ sung nhân vật khi thiếu hụt, đảm bảo tính hợp lý cá nhân đồng thời duy trì phân bố tổng thể. Đối với FPT Smart Cloud, khả năng tạo quần thể mô phỏng chính xác theo chủ đề sẽ nâng cao độ tin cậy của các nền tảng xã hội ảo, hệ thống đề xuất và mô hình kinh tế số, đồng thời giảm rủi ro sai lệch dữ liệu và chi phí thu thập thông tin thực tế.

**Ngày xuất bản:** 09 Jan, 2026
**URL:** https://arxiv.org/pdf/2601.05656v1

## Kết luận

Những tiến bộ vừa nêu không chỉ nâng cao tốc độ truyền dữ liệu và giảm tiêu thụ năng lượng mà còn tạo ra các tầng lưu trữ ngữ cảnh gần GPU, khả năng suy luận nhanh tại biên và môi trường an toàn cho đa‑tenant. Đối với FPT Smart Cloud, việc đánh giá và tích hợp ICMS/G3.5 sẽ giúp triển khai LLM siêu dài mà không tắc nghẽn GPU; áp dụng TensorRT Edge‑LLM mở ra dịch vụ edge cloud cho xe tự lái và robot; triển khai BlueField Astra cung cấp nền tảng mạng an toàn cho môi trường đa khách hàng; sử dụng khung Netomi agent tăng cường tính ổn định và quản trị trong chatbot doanh nghiệp; khai thác MICL và LoopBench để cải thiện độ tin cậy của ASR và tránh vòng lặp suy luận; tận dụng AGDC cho dịch vụ thiết kế vi mạch tự động; cùng với Stephanie2 và HAG nâng cao trải nghiệm hội thoại và mô phỏng người dùng chính xác hơn. Khi chúng ta biến những công nghệ này thành hành động thực tiễn, FPT Smart Cloud sẽ củng cố vị thế dẫn đầu trong thị trường đám mây AI nội địa và quốc tế, đồng thời đáp ứng nhu cầu ngày càng đa dạng của khách hàng doanh nghiệp.

| Tiêu đề | Ngày xuất bản | URL |
|---|---|---|
| Introducing NVIDIA BlueField-4-Powered Inference Context Memory Storage Platform for the Next Frontier of AI | 06 Jan, 2026 | [Link](https://developer.nvidia.com/blog/introducing-nvidia-bluefield-4-powered-inference-context-memory-storage-platform-for-the-next-frontier-of-ai/) |
| Netomi’s lessons for scaling agentic systems into the enterprise | 08 Jan, 2026 | [Link](https://openai.com/index/netomi) |
| Accelerating LLM and VLM Inference for Automotive and Robotics with NVIDIA TensorRT Edge-LLM | 08 Jan, 2026 | [Link](https://developer.nvidia.com/blog/accelerating-llm-and-vlm-inference-for-automotive-and-robotics-with-nvidia-tensorrt-edge-llm/) |
| Scaling Power-Efficient AI Factories with NVIDIA Spectrum-X Ethernet Photonics | 06 Jan, 2026 | [Link](https://developer.nvidia.com/blog/scaling-power-efficient-ai-factories-with-nvidia-spectrum-x-ethernet-photonics/) |
| Delivering Massive Performance Leaps for Mixture of Experts Inference on NVIDIA Blackwell | 08 Jan, 2026 | [Link](https://developer.nvidia.com/blog/delivering-massive-performance-leaps-for-mixture-of-experts-inference-on-nvidia-blackwell/) |
| Redefining Secure AI Infrastructure with NVIDIA BlueField Astra for NVIDIA Vera Rubin NVL72 | 07 Jan, 2026 | [Link](https://developer.nvidia.com/blog/redefining-secure-ai-infrastructure-with-nvidia-bluefield-astra-for-nvidia-vera-rubin-nvl72/) |
| Introducing NVIDIA BlueField-4-Powered Inference Context Memory Storage Platform for the Next Frontier of AI | 06 Jan, 2026 | [Link](https://developer.nvidia.com/blog/introducing-nvidia-bluefield-4-powered-inference-context-memory-storage-platform-for-the-next-frontier-of-ai/) |
| Redefining Secure AI Infrastructure with NVIDIA BlueField Astra for NVIDIA Vera Rubin NVL72 | 07 Jan, 2026 | [Link](https://developer.nvidia.com/blog/redefining-secure-ai-infrastructure-with-nvidia-bluefield-astra-for-nvidia-vera-rubin-nvl72/) |
| Introducing NVIDIA BlueField-4-Powered Inference Context Memory Storage Platform for the Next Frontier of AI | 06 Jan, 2026 | [Link](https://developer.nvidia.com/blog/introducing-nvidia-bluefield-4-powered-inference-context-memory-storage-platform-for-the-next-frontier-of-ai/) |
| Scaling Power-Efficient AI Factories with NVIDIA Spectrum-X Ethernet Photonics | 06 Jan, 2026 | [Link](https://developer.nvidia.com/blog/scaling-power-efficient-ai-factories-with-nvidia-spectrum-x-ethernet-photonics/) |
| Multimodal In-context Learning for ASR of Low-resource Languages | 09 Jan, 2026 | [Link](https://arxiv.org/pdf/2601.05707v1) |
| Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models | 09 Jan, 2026 | [Link](https://arxiv.org/pdf/2601.05693v1) |
| AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces | 09 Jan, 2026 | [Link](https://arxiv.org/pdf/2601.05680v1) |
| Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat | 09 Jan, 2026 | [Link](https://arxiv.org/pdf/2601.05657v1) |
| HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation | 09 Jan, 2026 | [Link](https://arxiv.org/pdf/2601.05656v1) |
---
Bản tin được tạo tự động bởi hệ thống FCI News Agents.
